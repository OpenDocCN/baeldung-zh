["```\r\n bin/kafka-topics.sh --create \\\r\n  --zookeeper localhost:2181 \\\r\n  --replication-factor 1 --partitions 1 \\\r\n  --topic flink_output\r\n\r\n bin/kafka-topics.sh --create \\\r\n  --zookeeper localhost:2181 \\\r\n  --replication-factor 1 --partitions 1 \\\r\n  --topic flink_input\r\n```","```\r\n<dependency>\r\n    <groupId>org.apache.flink</groupId>\r\n    <artifactId>flink-core</artifactId>\r\n    <version>1.5.0</version>\r\n</dependency>\r\n<dependency>\r\n    <groupId>org.apache.flink</groupId>\r\n    <artifactId>flink-connector-kafka-0.11_2.11</artifactId>\r\n    <version>1.5.0</version>\r\n</dependency>\r\n```","```\r\npublic static FlinkKafkaConsumer011<String> createStringConsumerForTopic(\r\n  String topic, String kafkaAddress, String kafkaGroup ) {\r\n\r\n    Properties props = new Properties();\r\n    props.setProperty(\"bootstrap.servers\", kafkaAddress);\r\n    props.setProperty(\"group.id\",kafkaGroup);\r\n    FlinkKafkaConsumer011<String> consumer = new FlinkKafkaConsumer011<>(\r\n      topic, new SimpleStringSchema(), props);\r\n\r\n    return consumer;\r\n}\r\n```","```\r\npublic static FlinkKafkaProducer011<String> createStringProducer(\r\n  String topic, String kafkaAddress){\r\n\r\n    return new FlinkKafkaProducer011<>(kafkaAddress,\r\n      topic, new SimpleStringSchema());\r\n}\r\n```","```\r\npublic class WordsCapitalizer implements MapFunction<String, String> {\r\n    @Override\r\n    public String map(String s) {\r\n        return s.toUpperCase();\r\n    }\r\n}\r\n```","```\r\npublic static void capitalize() {\r\n    String inputTopic = \"flink_input\";\r\n    String outputTopic = \"flink_output\";\r\n    String consumerGroup = \"baeldung\";\r\n    String address = \"localhost:9092\";\r\n    StreamExecutionEnvironment environment = StreamExecutionEnvironment\r\n      .getExecutionEnvironment();\r\n    FlinkKafkaConsumer011<String> flinkKafkaConsumer = createStringConsumerForTopic(\r\n      inputTopic, address, consumerGroup);\r\n    DataStream<String> stringInputStream = environment\r\n      .addSource(flinkKafkaConsumer);\r\n\r\n    FlinkKafkaProducer011<String> flinkKafkaProducer = createStringProducer(\r\n      outputTopic, address);\r\n\r\n    stringInputStream\r\n      .map(new WordsCapitalizer())\r\n      .addSink(flinkKafkaProducer);\r\n}\r\n```","```\r\n@JsonSerialize\r\npublic class InputMessage {\r\n    String sender;\r\n    String recipient;\r\n    LocalDateTime sentAt;\r\n    String message;\r\n}\r\n```","```\r\npublic class InputMessageDeserializationSchema implements\r\n  DeserializationSchema<InputMessage> {\r\n\r\n    static ObjectMapper objectMapper = new ObjectMapper()\r\n      .registerModule(new JavaTimeModule());\r\n\r\n    @Override\r\n    public InputMessage deserialize(byte[] bytes) throws IOException {\r\n        return objectMapper.readValue(bytes, InputMessage.class);\r\n    }\r\n\r\n    @Override\r\n    public boolean isEndOfStream(InputMessage inputMessage) {\r\n        return false;\r\n    }\r\n\r\n    @Override\r\n    public TypeInformation<InputMessage> getProducedType() {\r\n        return TypeInformation.of(InputMessage.class);\r\n    }\r\n}\r\n```","```\r\npublic class Backup {\r\n    @JsonProperty(\"inputMessages\")\r\n    List<InputMessage> inputMessages;\r\n    @JsonProperty(\"backupTimestamp\")\r\n    LocalDateTime backupTimestamp;\r\n    @JsonProperty(\"uuid\")\r\n    UUID uuid;\r\n\r\n    public Backup(List<InputMessage> inputMessages, \r\n      LocalDateTime backupTimestamp) {\r\n        this.inputMessages = inputMessages;\r\n        this.backupTimestamp = backupTimestamp;\r\n        this.uuid = UUID.randomUUID();\r\n    }\r\n}\r\n```","```\r\npublic class BackupSerializationSchema\r\n  implements SerializationSchema<Backup> {\r\n\r\n    ObjectMapper objectMapper;\r\n    Logger logger = LoggerFactory.getLogger(BackupSerializationSchema.class);\r\n\r\n    @Override\r\n    public byte[] serialize(Backup backupMessage) {\r\n        if(objectMapper == null) {\r\n            objectMapper = new ObjectMapper()\r\n              .registerModule(new JavaTimeModule());\r\n        }\r\n        try {\r\n            return objectMapper.writeValueAsString(backupMessage).getBytes();\r\n        } catch (com.fasterxml.jackson.core.JsonProcessingException e) {\r\n            logger.error(\"Failed to parse JSON\", e);\r\n        }\r\n        return new byte[0];\r\n    }\r\n}\r\n```","```\r\npublic class InputMessageTimestampAssigner \r\n  implements AssignerWithPunctuatedWatermarks<InputMessage> {\r\n\r\n    @Override\r\n    public long extractTimestamp(InputMessage element, \r\n      long previousElementTimestamp) {\r\n        ZoneId zoneId = ZoneId.systemDefault();\r\n        return element.getSentAt().atZone(zoneId).toEpochSecond() * 1000;\r\n    }\r\n\r\n    @Nullable\r\n    @Override\r\n    public Watermark checkAndGetNextWatermark(InputMessage lastElement, \r\n      long extractedTimestamp) {\r\n        return new Watermark(extractedTimestamp - 1500);\r\n    }\r\n}\r\n```","```\r\npublic class BackupAggregator \r\n  implements AggregateFunction<InputMessage, List<InputMessage>, Backup> {\r\n\r\n    @Override\r\n    public List<InputMessage> createAccumulator() {\r\n        return new ArrayList<>();\r\n    }\r\n\r\n    @Override\r\n    public List<InputMessage> add(\r\n      InputMessage inputMessage,\r\n      List<InputMessage> inputMessages) {\r\n        inputMessages.add(inputMessage);\r\n        return inputMessages;\r\n    }\r\n\r\n    @Override\r\n    public Backup getResult(List<InputMessage> inputMessages) {\r\n        return new Backup(inputMessages, LocalDateTime.now());\r\n    }\r\n\r\n    @Override\r\n    public List<InputMessage> merge(List<InputMessage> inputMessages,\r\n      List<InputMessage> acc1) {\r\n        inputMessages.addAll(acc1);\r\n        return inputMessages;\r\n    }\r\n}\r\n```","```\r\npublic static void createBackup () throws Exception {\r\n    String inputTopic = \"flink_input\";\r\n    String outputTopic = \"flink_output\";\r\n    String consumerGroup = \"baeldung\";\r\n    String kafkaAddress = \"192.168.99.100:9092\";\r\n    StreamExecutionEnvironment environment\r\n      = StreamExecutionEnvironment.getExecutionEnvironment();\r\n    environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\r\n    FlinkKafkaConsumer011<InputMessage> flinkKafkaConsumer\r\n      = createInputMessageConsumer(inputTopic, kafkaAddress, consumerGroup);\r\n    flinkKafkaConsumer.setStartFromEarliest();\r\n\r\n    flinkKafkaConsumer.assignTimestampsAndWatermarks(\r\n      new InputMessageTimestampAssigner());\r\n    FlinkKafkaProducer011<Backup> flinkKafkaProducer\r\n      = createBackupProducer(outputTopic, kafkaAddress);\r\n\r\n    DataStream<InputMessage> inputMessagesStream\r\n      = environment.addSource(flinkKafkaConsumer);\r\n\r\n    inputMessagesStream\r\n      .timeWindowAll(Time.hours(24))\r\n      .aggregate(new BackupAggregator())\r\n      .addSink(flinkKafkaProducer);\r\n\r\n    environment.execute();\r\n}\r\n```"]
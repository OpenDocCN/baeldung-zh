["```\r\n<dependency>\r\n    <groupId>org.apache.spark</groupId>\r\n    <artifactId>spark-core_2.11</artifactId>\r\n    <version>2.4.8</version>\r\n</dependency>\r\n\r\n<dependency>\r\n    <groupId>org.apache.spark</groupId>\r\n    <artifactId>spark-sql_2.11</artifactId>\r\n    <version>2.4.8</version>\r\n</dependency> \r\n```","```\r\npublic static StructType minimumCustomerDataSchema() {\r\n    return DataTypes.createStructType(new StructField[] {\r\n      DataTypes.createStructField(\"id\", DataTypes.StringType, true),\r\n      DataTypes.createStructField(\"name\", DataTypes.StringType, true),\r\n      DataTypes.createStructField(\"gender\", DataTypes.StringType, true),\r\n      DataTypes.createStructField(\"transaction_amount\", DataTypes.IntegerType, true) }\r\n    );\r\n}\r\n```","```\r\npublic static SparkSession getSparkSession() {\r\n    return SparkSession.builder()\r\n      .appName(\"Customer Aggregation pipeline\")\r\n      .master(\"local\")\r\n      .getOrCreate();\r\n}\r\n```","```\r\nList<Customer> customers = Arrays.asList(\r\n  aCustomerWith(\"01\", \"jo\", \"Female\", 2000), \r\n  aCustomerWith(\"02\", \"jack\", \"Male\", 1200)\r\n);\r\n```","```\r\nDataset<Row> df = SPARK_SESSION\r\n  .createDataFrame(customerList, Customer.class);\r\n```","```\r\nDataset<Customer> customerPOJODataSet = SPARK_SESSION\r\n  .createDataset(CUSTOMERS, Encoders.bean(Customer.class));\r\n```","```\r\nDataset<Row> df = customerPOJODataSet.toDF();\r\n```","```\r\npublic class CustomerToRowMapper implements MapFunction<Customer, Row> {\r\n\r\n    @Override\r\n    public Row call(Customer customer) throws Exception {\r\n        Row row = RowFactory.create(\r\n          customer.getId(),\r\n          customer.getName().toUpperCase(),\r\n          StringUtils.substring(customer.getGender(),0, 1),\r\n          customer.getTransaction_amount()\r\n        );\r\n        return row;\r\n    }\r\n}\r\n```","```\r\nList<Row> rows = customer.stream()\r\n  .map(c -> new CustomerToRowMapper().call(c))\r\n  .collect(Collectors.toList());\r\n```","```\r\nDataset<Row> df = SparkDriver.getSparkSession()\r\n  .createDataFrame(rows, SchemaFactory.minimumCustomerDataSchema());\r\n```","```\r\nDataset<Row> df = SparkDriver.getSparkSession()\r\n  .read()\r\n  .format(\"org.apache.spark.sql.execution.datasources.json.JsonFileFormat\")\r\n  .option(\"multiline\", true)\r\n  .load(\"data/minCustomerData.json\");\r\n```","```\r\nDataset<Row> df = SparkDriver.getSparkSession()\r\n  .read()\r\n  .option(\"url\", \"jdbc:postgresql://localhost:5432/customerdb\")\r\n  .option(\"dbtable\", \"customer\")\r\n  .option(\"user\", \"user\")\r\n  .option(\"password\", \"password\")\r\n  .option(\"serverTimezone\", \"EST\")\r\n  .format(\"jdbc\")\r\n  .load();\r\n```","```\r\nDataset<Customer> ds = df.map(\r\n  new CustomerMapper(),\r\n  Encoders.bean(Customer.class)\r\n);\r\n```","```\r\npublic class CustomerMapper implements MapFunction<Row, Customer> {\r\n\r\n    @Override\r\n    public Customer call(Row row) {\r\n        Customer customer = new Customer();\r\n        customer.setId(row.getAs(\"id\"));\r\n        customer.setName(row.getAs(\"name\"));\r\n        customer.setGender(row.getAs(\"gender\"));\r\n        customer.setTransaction_amount(Math.toIntExact(row.getAs(\"transaction_amount\")));\r\n        return customer;\r\n    }\r\n}\r\n```","```\r\nDataset<Row> jsonDataToDF = SPARK_SESSION.read()\r\n  .format(\"org.apache.spark.sql.execution.datasources.json.JsonFileFormat\")\r\n  .option(\"multiline\", true)\r\n  .load(\"data/customerData.json\"); \r\n```","```\r\nDataset<Row> csvDataToDF = SPARK_SESSION.read()\r\n  .format(\"csv\")\r\n  .option(\"header\", \"true\")\r\n  .schema(SchemaFactory.customerSchema())\r\n  .option(\"dateFormat\", \"m/d/YYYY\")\r\n  .load(\"data/customerData.csv\"); \r\n\r\ncsvDataToDF.show(); \r\ncsvDataToDF.printSchema(); \r\nreturn csvData;\r\n```","```\r\nprivate Dataset<Row> normalizeCustomerDataFromEbay(Dataset<Row> rawDataset) {\r\n    Dataset<Row> transformedDF = rawDataset\r\n      .withColumn(\"id\", concat(rawDataset.col(\"zoneId\"),lit(\"-\"), rawDataset.col(\"customerId\")))\r\n      .drop(column(\"customerId\"))\r\n      .withColumn(\"source\", lit(\"ebay\"))\r\n      .withColumn(\"city\", rawDataset.col(\"contact.customer_city\"))\r\n      .drop(column(\"contact\"))\r\n      .drop(column(\"zoneId\"))\r\n      .withColumn(\"year\", functions.year(col(\"transaction_date\")))\r\n      .drop(\"transaction_date\")\r\n      .withColumn(\"firstName\", functions.split(column(\"name\"), \" \")\r\n        .getItem(0))\r\n      .withColumn(\"lastName\", functions.split(column(\"name\"), \" \")\r\n        .getItem(1))\r\n      .drop(column(\"name\"));\r\n\r\n    return transformedDF; \r\n}\r\n```","```\r\nroot\r\n |-- gender: string (nullable = true)\r\n |-- transaction_amount: long (nullable = true)\r\n |-- id: string (nullable = true)\r\n |-- source: string (nullable = false)\r\n |-- city: string (nullable = true)\r\n |-- year: integer (nullable = true)\r\n |-- firstName: string (nullable = true)\r\n |-- lastName: string (nullable = true)\r\n```","```\r\nDataset<Row> combineDataframes(Dataset<Row> df1, Dataset<Row> df2) {\r\n    return df1.unionByName(df2); \r\n}\r\n```","```\r\nDataset<Row> aggDF = dataset\r\n  .groupBy(column(\"year\"), column(\"source\"), column(\"gender\"))\r\n  .sum(\"transactionAmount\")\r\n  .withColumnRenamed(\"sum(transaction_amount)\", \"yearly spent\")\r\n  .orderBy(col(\"year\").asc(), col(\"yearly spent\").desc());\r\n```","```\r\n+----+------+------+---------------+\r\n|year|source|gender|annual_spending|\r\n+----+------+------+---------------+\r\n|2018|amazon|  Male|          10600|\r\n|2018|amazon|Female|           6200|\r\n|2018|  ebay|  Male|           5500|\r\n|2021|  ebay|Female|          16000|\r\n|2021|  ebay|  Male|          13500|\r\n|2021|amazon|  Male|           4000|\r\n|2021|amazon|Female|           2000|\r\n+----+------+------+---------------+\r\n```","```\r\nroot\r\n |-- source: string (nullable = false)\r\n |-- gender: string (nullable = true)\r\n |-- year: integer (nullable = true)\r\n |-- yearly spent: long (nullable = true) \r\n```","```\r\nProperties dbProps = new Properties();\r\n\r\ndbProps.setProperty(\"connectionURL\", \"jdbc:postgresql://localhost:5432/customerdb\");\r\ndbProps.setProperty(\"driver\", \"org.postgresql.Driver\");\r\ndbProps.setProperty(\"user\", \"postgres\");\r\ndbProps.setProperty(\"password\", \"postgres\");\r\n```","```\r\nString connectionURL = dbProperties.getProperty(\"connectionURL\");\r\n\r\ndataset.write()\r\n  .mode(SaveMode.Overwrite)\r\n  .jdbc(connectionURL, \"customer\", dbProperties);\r\n```","```\r\n@Test\r\nvoid givenCSVAndJSON_whenRun_thenStoresAggregatedDataFrameInDB() throws Exception {\r\n    Properties dbProps = new Properties();\r\n    dbProps.setProperty(\"connectionURL\", \"jdbc:postgresql://localhost:5432/customerdb\");\r\n    dbProps.setProperty(\"driver\", \"org.postgresql.Driver\");\r\n    dbProps.setProperty(\"user\", \"postgres\");\r\n    dbProps.setProperty(\"password\", \"postgres\");\r\n\r\n    pipeline = new CustomerDataAggregationPipeline(dbProps);\r\n    pipeline.run();\r\n\r\n    String allCustomersSql = \"Select count(*) from customer\";\r\n\r\n    Statement statement = conn.createStatement();\r\n    ResultSet resultSet = statement.executeQuery(allCustomersSql);\r\n    resultSet.next();\r\n    int count = resultSet.getInt(1);\r\n    assertEquals(7, count);\r\n}\r\n```"]
["```\r\n<dependency>\r\n    <groupId>edu.uci.ics</groupId>\r\n    <artifactId>crawler4j</artifactId>\r\n    <version>4.4.0</version>\r\n</dependency>\r\n```","```\r\npublic class HtmlCrawler extends WebCrawler {\r\n\r\n    private final static Pattern EXCLUSIONS\r\n      = Pattern.compile(\".*(\\\\.(css|js|xml|gif|jpg|png|mp3|mp4|zip|gz|pdf))$\");\r\n\r\n    // more code\r\n}\r\n```","```\r\n@Override\r\npublic boolean shouldVisit(Page referringPage, WebURL url) {\r\n    String urlString = url.getURL().toLowerCase();\r\n    return !EXCLUSIONS.matcher(urlString).matches() \r\n      && urlString.startsWith(\"https://www.baeldung.com/\");\r\n}\r\n```","```\r\n@Override\r\npublic void visit(Page page) {\r\n    String url = page.getWebURL().getURL();\r\n\r\n    if (page.getParseData() instanceof HtmlParseData) {\r\n        HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();\r\n        String title = htmlParseData.getTitle();\r\n        String text = htmlParseData.getText();\r\n        String html = htmlParseData.getHtml();\r\n        Set<WebURL> links = htmlParseData.getOutgoingUrls();\r\n\r\n        // do something with the collected data\r\n    }\r\n}\r\n```","```\r\nFile crawlStorage = new File(\"src/test/resources/crawler4j\");\r\nCrawlConfig config = new CrawlConfig();\r\nconfig.setCrawlStorageFolder(crawlStorage.getAbsolutePath());\r\n\r\nint numCrawlers = 12;\r\n\r\nPageFetcher pageFetcher = new PageFetcher(config);\r\nRobotstxtConfig robotstxtConfig = new RobotstxtConfig();\r\nRobotstxtServer robotstxtServer= new RobotstxtServer(robotstxtConfig, pageFetcher);\r\nCrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);\r\n\r\ncontroller.addSeed(\"https://www.baeldung.com/\");\r\n\r\nCrawlController.WebCrawlerFactory<HtmlCrawler> factory = HtmlCrawler::new;\r\n\r\ncontroller.start(factory, numCrawlers);\r\n```","```\r\npublic class ImageCrawler extends WebCrawler {\r\n    private final static Pattern EXCLUSIONS\r\n      = Pattern.compile(\".*(\\\\.(css|js|xml|gif|png|mp3|mp4|zip|gz|pdf))$\");\r\n\r\n    private static final Pattern IMG_PATTERNS = Pattern.compile(\".*(\\\\.(jpg|jpeg))$\");\r\n\r\n    private File saveDir;\r\n\r\n    public ImageCrawler(File saveDir) {\r\n        this.saveDir = saveDir;\r\n    }\r\n\r\n    // more code\r\n\r\n}\r\n```","```\r\n@Override\r\npublic boolean shouldVisit(Page referringPage, WebURL url) {\r\n    String urlString = url.getURL().toLowerCase();\r\n    if (EXCLUSIONS.matcher(urlString).matches()) {\r\n        return false;\r\n    }\r\n\r\n    if (IMG_PATTERNS.matcher(urlString).matches() \r\n        || urlString.startsWith(\"https://www.baeldung.com/\")) {\r\n        return true;\r\n    }\r\n\r\n    return false;\r\n}\r\n```","```\r\n@Override\r\npublic void visit(Page page) {\r\n    String url = page.getWebURL().getURL();\r\n    if (IMG_PATTERNS.matcher(url).matches() \r\n        && page.getParseData() instanceof BinaryParseData) {\r\n        String extension = url.substring(url.lastIndexOf(\".\"));\r\n        int contentLength = page.getContentData().length;\r\n\r\n        // write the content data to a file in the save directory\r\n    }\r\n}\r\n```","```\r\nCrawlConfig config = new CrawlConfig();\r\nconfig.setIncludeBinaryContentInCrawling(true);\r\n\r\n// ... same as before\r\n\r\nCrawlController.WebCrawlerFactory<ImageCrawler> factory = () -> new ImageCrawler(saveDir);\r\n\r\ncontroller.start(factory, numCrawlers);\r\n```","```\r\npublic class CrawlerStatistics {\r\n    private int processedPageCount = 0;\r\n    private int totalLinksCount = 0;\r\n\r\n    public void incrementProcessedPageCount() {\r\n        processedPageCount++;\r\n    }\r\n\r\n    public void incrementTotalLinksCount(int linksCount) {\r\n        totalLinksCount += linksCount;\r\n    }\r\n\r\n    // standard getters\r\n}\r\n```","```\r\nprivate CrawlerStatistics stats;\r\n\r\npublic HtmlCrawler(CrawlerStatistics stats) {\r\n    this.stats = stats;\r\n}\r\n```","```\r\n@Override\r\npublic void visit(Page page) {\r\n    String url = page.getWebURL().getURL();\r\n    stats.incrementProcessedPageCount();\r\n\r\n    if (page.getParseData() instanceof HtmlParseData) {\r\n        HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();\r\n        String title = htmlParseData.getTitle();\r\n        String text = htmlParseData.getText();\r\n        String html = htmlParseData.getHtml();\r\n        Set<WebURL> links = htmlParseData.getOutgoingUrls();\r\n        stats.incrementTotalLinksCount(links.size());\r\n\r\n        // do something with collected data\r\n    }\r\n}\r\n```","```\r\nCrawlerStatistics stats = new CrawlerStatistics();\r\nCrawlController.WebCrawlerFactory<HtmlCrawler> factory = () -> new HtmlCrawler(stats);\r\n```","```\r\nFile crawlStorageBase = new File(\"src/test/resources/crawler4j\");\r\nCrawlConfig htmlConfig = new CrawlConfig();\r\nCrawlConfig imageConfig = new CrawlConfig();\r\n\r\n// Configure storage folders and other configurations\r\n\r\nPageFetcher pageFetcherHtml = new PageFetcher(htmlConfig);\r\nPageFetcher pageFetcherImage = new PageFetcher(imageConfig);\r\n\r\nRobotstxtConfig robotstxtConfig = new RobotstxtConfig();\r\nRobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcherHtml);\r\n\r\nCrawlController htmlController\r\n  = new CrawlController(htmlConfig, pageFetcherHtml, robotstxtServer);\r\nCrawlController imageController\r\n  = new CrawlController(imageConfig, pageFetcherImage, robotstxtServer);\r\n\r\n// add seed URLs\r\n\r\nCrawlerStatistics stats = new CrawlerStatistics();\r\nCrawlController.WebCrawlerFactory<HtmlCrawler> htmlFactory = () -> new HtmlCrawler(stats);\r\n\r\nFile saveDir = new File(\"src/test/resources/crawler4j\");\r\nCrawlController.WebCrawlerFactory<ImageCrawler> imageFactory\r\n  = () -> new ImageCrawler(saveDir);\r\n\r\nimageController.startNonBlocking(imageFactory, 7);\r\nhtmlController.startNonBlocking(htmlFactory, 10);\r\n\r\nhtmlController.waitUntilFinish();\r\nimageController.waitUntilFinish();\r\n```","```\r\ncrawlConfig.setMaxDepthOfCrawling(2);\r\n```","```\r\ncrawlConfig.setMaxPagesToFetch(500);\r\n```","```\r\ncrawlConfig.setMaxOutgoingLinksToFollow(2000);\r\n```","```\r\ncrawlConfig.setPolitenessDelay(300);\r\n```","```\r\ncrawlConfig.setIncludeBinaryContentInCrawling(true);\r\n```","```\r\ncrawlConfig.setIncludeHttpsPages(false);\r\n```","```\r\ncrawlConfig.setResumableCrawling(true);\r\n```","```\r\ncrawlConfig.setUserAgentString(\"baeldung demo (https://github.com/yasserg/crawler4j/)\");\r\n```"]
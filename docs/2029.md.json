["```\r\n $KAFKA_HOME$\\bin\\windows\\kafka-topics.bat --create \\\r\n  --zookeeper localhost:2181 \\\r\n  --replication-factor 1 --partitions 1 \\\r\n  --topic messages\r\n```","```\r\nCREATE KEYSPACE vocabulary\r\n    WITH REPLICATION = {\r\n        'class' : 'SimpleStrategy',\r\n        'replication_factor' : 1\r\n    };\r\nUSE vocabulary;\r\nCREATE TABLE words (word text PRIMARY KEY, count int);\r\n```","```\r\n<dependency>\r\n    <groupId>org.apache.spark</groupId>\r\n    <artifactId>spark-core_2.11</artifactId>\r\n    <version>2.3.0</version>\r\n    <scope>provided</scope>\r\n</dependency>\r\n<dependency>\r\n    <groupId>org.apache.spark</groupId>\r\n    <artifactId>spark-sql_2.11</artifactId>\r\n    <version>2.3.0</version>\r\n    <scope>provided</scope>\r\n</dependency>\r\n<dependency>\r\n    <groupId>org.apache.spark</groupId>\r\n    <artifactId>spark-streaming_2.11</artifactId>\r\n    <version>2.3.0</version>\r\n    <scope>provided</scope>\r\n</dependency>\r\n<dependency>\r\n    <groupId>org.apache.spark</groupId>\r\n    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>\r\n    <version>2.3.0</version>\r\n</dependency>\r\n<dependency>\r\n    <groupId>com.datastax.spark</groupId>\r\n    <artifactId>spark-cassandra-connector_2.11</artifactId>\r\n    <version>2.3.0</version>\r\n</dependency>\r\n<dependency>\r\n    <groupId>com.datastax.spark</groupId>\r\n    <artifactId>spark-cassandra-connector-java_2.11</artifactId>\r\n    <version>1.5.2</version>\r\n</dependency>\r\n```","```\r\nSparkConf sparkConf = new SparkConf();\r\nsparkConf.setAppName(\"WordCountingApp\");\r\nsparkConf.set(\"spark.cassandra.connection.host\", \"127.0.0.1\");\r\n\r\nJavaStreamingContext streamingContext = new JavaStreamingContext(\r\n  sparkConf, Durations.seconds(1));\r\n```","```\r\nMap<String, Object> kafkaParams = new HashMap<>();\r\nkafkaParams.put(\"bootstrap.servers\", \"localhost:9092\");\r\nkafkaParams.put(\"key.deserializer\", StringDeserializer.class);\r\nkafkaParams.put(\"value.deserializer\", StringDeserializer.class);\r\nkafkaParams.put(\"group.id\", \"use_a_separate_group_id_for_each_stream\");\r\nkafkaParams.put(\"auto.offset.reset\", \"latest\");\r\nkafkaParams.put(\"enable.auto.commit\", false);\r\nCollection<String> topics = Arrays.asList(\"messages\");\r\n\r\nJavaInputDStream<ConsumerRecord<String, String>> messages = \r\n  KafkaUtils.createDirectStream(\r\n    streamingContext, \r\n    LocationStrategies.PreferConsistent(), \r\n    ConsumerStrategies.<String, String> Subscribe(topics, kafkaParams));\r\n```","```\r\nJavaPairDStream<String, String> results = messages\r\n  .mapToPair( \r\n      record -> new Tuple2<>(record.key(), record.value())\r\n  );\r\nJavaDStream<String> lines = results\r\n  .map(\r\n      tuple2 -> tuple2._2()\r\n  );\r\nJavaDStream<String> words = lines\r\n  .flatMap(\r\n      x -> Arrays.asList(x.split(\"\\\\s+\")).iterator()\r\n  );\r\nJavaPairDStream<String, Integer> wordCounts = words\r\n  .mapToPair(\r\n      s -> new Tuple2<>(s, 1)\r\n  ).reduceByKey(\r\n      (i1, i2) -> i1 + i2\r\n    );\r\n```","```\r\nwordCounts.foreachRDD(\r\n    javaRdd -> {\r\n      Map<String, Integer> wordCountMap = javaRdd.collectAsMap();\r\n      for (String key : wordCountMap.keySet()) {\r\n        List<Word> wordList = Arrays.asList(new Word(key, wordCountMap.get(key)));\r\n        JavaRDD<Word> rdd = streamingContext.sparkContext().parallelize(wordList);\r\n        javaFunctions(rdd).writerBuilder(\r\n          \"vocabulary\", \"words\", mapToRow(Word.class)).saveToCassandra();\r\n      }\r\n    }\r\n  );\r\n```","```\r\nstreamingContext.start();\r\nstreamingContext.awaitTermination();\r\n```","```\r\nstreamingContext.checkpoint(\"./.checkpoint\");\r\n```","```\r\nJavaMapWithStateDStream<String, Integer, Integer, Tuple2<String, Integer>> cumulativeWordCounts = wordCounts\r\n  .mapWithState(\r\n    StateSpec.function( \r\n        (word, one, state) -> {\r\n          int sum = one.orElse(0) + (state.exists() ? state.get() : 0);\r\n          Tuple2<String, Integer> output = new Tuple2<>(word, sum);\r\n          state.update(sum);\r\n          return output;\r\n        }\r\n      )\r\n    );\r\n```","```\r\nkafkaParams.put(\"auto.offset.reset\", \"latest\");\r\nkafkaParams.put(\"enable.auto.commit\", false);\r\n```","```\r\n$SPARK_HOME$\\bin\\spark-submit \\\r\n  --class com.baeldung.data.pipeline.WordCountingAppWithCheckpoint \\\r\n  --master local[2] \r\n  \\target\\spark-streaming-app-0.0.1-SNAPSHOT-jar-with-dependencies.jar\r\n```"]
["```\r\nSparkSession session = SparkSession.builder()\r\n  .appName(\"TouristDataFrameExample\")\r\n  .master(\"local[*]\")\r\n  .getOrCreate();\r\n\r\nDataFrameReader dataFrameReader = session.read();\r\n```","```\r\nDataset<Row> data = dataFrameReader.option(\"header\", \"true\")\r\n  .csv(\"data/Tourist.csv\");\r\n```","```\r\ndata.select(col(\"country\"), col(\"year\"), col(\"value\"))\r\n  .show();\r\n\r\ndata.filter(col(\"country\").equalTo(\"Mexico\"))\r\n  .show();\r\n\r\ndata.groupBy(col(\"country\"))\r\n  .count()\r\n  .show();\r\n```","```\r\npublic class TouristData {\r\n    private String region;\r\n    private String country;\r\n    private String year;\r\n    private String series;\r\n    private Double value;\r\n    private String footnotes;\r\n    private String source;\r\n    // ... getters and setters\r\n}\r\n```","```\r\n// SparkSession initialization and data load\r\nDataset<Row> responseWithSelectedColumns = data.select(col(\"region\"), \r\n  col(\"country\"), col(\"year\"), col(\"series\"), col(\"value\").cast(\"double\"), \r\n  col(\"footnotes\"), col(\"source\"));\r\n\r\nDataset<TouristData> typedDataset = responseWithSelectedColumns\r\n  .as(Encoders.bean(TouristData.class));\r\n```","```\r\ntypedDataset.filter((FilterFunction) record -> record.getCountry()\r\n  .equals(\"Norway\"))\r\n  .show();\r\n\r\ntypedDataset.groupBy(typedDataset.col(\"country\"))\r\n  .count()\r\n  .show();\r\n```","```\r\ntypedDataset.filter((FilterFunction) record -> record.getYear() != null \r\n  && (Long.valueOf(record.getYear()) > 2010 \r\n  && Long.valueOf(record.getYear()) < 2017)).show();\r\n\r\ntypedDataset.filter((FilterFunction) record -> record.getValue() != null \r\n  && record.getSeries()\r\n    .contains(\"expenditure\"))\r\n    .groupBy(\"country\")\r\n    .agg(sum(\"value\"))\r\n    .show();\r\n```","```\r\nSparkConf conf = new SparkConf().setAppName(\"uppercaseCountries\")\r\n  .setMaster(\"local[*]\");\r\nJavaSparkContext sc = new JavaSparkContext(conf);\r\n\r\nJavaRDD<String> tourists = sc.textFile(\"data/Tourist.csv\");\r\n```","```\r\nJavaRDD<String> upperCaseCountries = tourists.map(line -> {\r\n    String[] columns = line.split(COMMA_DELIMITER);\r\n    return columns[1].toUpperCase();\r\n}).distinct();\r\n\r\nupperCaseCountries.saveAsTextFile(\"data/output/uppercase.txt\");\r\n```","```\r\nJavaRDD<String> touristsInMexico = tourists\r\n  .filter(line -> line.split(COMMA_DELIMITER)[1].equals(\"Mexico\"));\r\n\r\ntouristsInMexico.saveAsTextFile(\"data/output/touristInMexico.txt\");\r\n```","```\r\n// Spark Context initialization and data load\r\nJavaRDD<String> countries = tourists.map(line -> {\r\n    String[] columns = line.split(COMMA_DELIMITER);\r\n    return columns[1];\r\n}).distinct();\r\n\r\nLong numberOfCountries = countries.count();\r\n```","```\r\nJavaRDD<String> touristsExpenditure = tourists\r\n  .filter(line -> line.split(COMMA_DELIMITER)[3].contains(\"expenditure\"));\r\n\r\nJavaPairRDD<String, Double> expenditurePairRdd = touristsExpenditure\r\n  .mapToPair(line -> {\r\n      String[] columns = line.split(COMMA_DELIMITER);\r\n      return new Tuple2<>(columns[1], Double.valueOf(columns[6]));\r\n});\r\n\r\nList<Tuple2<String, Double>> totalByCountry = expenditurePairRdd\r\n  .reduceByKey((x, y) -> x + y)\r\n  .collect();\r\n```"]